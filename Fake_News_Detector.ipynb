{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/muskancodebase/skills-copilot-codespaces-vscode/blob/main/Fake_News_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WoUJVAi0BAR",
        "outputId": "4bd02d63-1f3d-4f56-ce4a-11a7889747e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers requests beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- All Necessary Imports ---\n",
        "import requests\n",
        "from transformers import pipeline\n",
        "from urllib.parse import urlparse\n",
        "!pip install trafilatura -q\n",
        "import trafilatura\n",
        "\n",
        "# --- 1. Define Authentic Sources ---\n",
        "# A set of reputable news sources and official institutions for fast lookups.\n",
        "AUTHENTIC_SOURCES = {\n",
        "    'reuters.com',\n",
        "    'apnews.com',\n",
        "    'bbc.com',\n",
        "    'aljazeera.com',\n",
        "    'dawn.com',\n",
        "    'tribune.com.pk',\n",
        "    'geo.tv',\n",
        "    'worldbank.org',  # Official institution\n",
        "    'sbp.org.pk'      # State Bank of Pakistan\n",
        "}\n",
        "\n",
        "# --- 2. Web Search Function ---\n",
        "def search_web(query):\n",
        "    \"\"\"\n",
        "    Searches the web and returns the top 5 result URLs.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(f\"https://html.duckduckgo.com/html/?q={query}\", headers=headers)\n",
        "        response.raise_for_status()\n",
        "        from bs4 import BeautifulSoup\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        links = [a['href'] for a in soup.find_all('a', class_='result__a')]\n",
        "        return links[:5]\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during web search: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- 3. Scrape and Summarize Function (with Trafilatura) ---\n",
        "def scrape_and_summarize(url):\n",
        "    \"\"\"\n",
        "    Scrapes content using trafilatura, chunks it, and summarizes it.\n",
        "    This version is more robust and uses the CPU to avoid CUDA errors.\n",
        "    \"\"\"\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        content = trafilatura.extract(response.text)\n",
        "\n",
        "        if not content or len(content.split()) < 50:\n",
        "            return None\n",
        "\n",
        "        # --- KEY CHANGE: Forcing the pipeline to use the CPU (device=-1) for stability ---\n",
        "        summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=-1)\n",
        "\n",
        "        tokenizer = summarizer.tokenizer\n",
        "        # Truncate the initial tokenization to a max length to prevent overload\n",
        "        tokens = tokenizer.encode(content, truncation=True, max_length=8192)\n",
        "\n",
        "        # The model's absolute max input is 1024, so we chunk below that\n",
        "        chunk_size = 1000\n",
        "\n",
        "        text_chunks = [\n",
        "            tokenizer.decode(tokens[i:i + chunk_size], skip_special_tokens=True)\n",
        "            for i in range(0, len(tokens), chunk_size)\n",
        "        ]\n",
        "\n",
        "        summaries = []\n",
        "        for chunk in text_chunks:\n",
        "            # Added a try-except block here for extra safety on a per-chunk basis\n",
        "            try:\n",
        "                chunk_summary = summarizer(chunk, max_length=100, min_length=20, do_sample=False)\n",
        "                summaries.append(chunk_summary[0]['summary_text'])\n",
        "            except Exception as e:\n",
        "                print(f\"    - Could not summarize a chunk. Error: {e}\")\n",
        "                continue # Move to the next chunk\n",
        "\n",
        "        if not summaries:\n",
        "            return None\n",
        "\n",
        "        combined_summary = \" \".join(summaries)\n",
        "\n",
        "        # Final summary of the combined summaries\n",
        "        final_summary = summarizer(combined_summary, max_length=150, min_length=30, do_sample=False)\n",
        "        return final_summary[0]['summary_text']\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  - Could not process {url[:50]}... Error: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "TLCfKNqf0HUz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_fake_news(article_text):\n",
        "    \"\"\"\n",
        "    Detects fake news and returns a verdict along with URLs from authentic sources as proof.\n",
        "    \"\"\"\n",
        "    print(\"--- Starting Intelligent Fake News Detection ---\")\n",
        "\n",
        "    # 1. Extract the main claim using a summarizer for accuracy\n",
        "    print(\"ðŸŽ¯ Extracting main claim from article...\")\n",
        "    # --- KEY CHANGE: Forcing this pipeline to use the CPU (device=-1) for stability ---\n",
        "    summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=-1)\n",
        "\n",
        "    main_claim = summarizer(article_text, max_length=60, min_length=15, do_sample=False)[0]['summary_text']\n",
        "    print(f\"ðŸŽ¯ Main Claim Identified: \\\"{main_claim}\\\"\")\n",
        "\n",
        "    # 2. Search the web\n",
        "    print(\"\\nðŸŒ Searching the web for evidence...\")\n",
        "    search_urls = search_web(main_claim)\n",
        "    if not search_urls:\n",
        "        return {\"verdict\": \"Uncertain (Web Search Failed)\", \"proof_urls\": []}\n",
        "\n",
        "    # 3. Scrape, summarize, and track successful sources\n",
        "    print(\"\\nâœï¸ Summarizing evidence from top sources...\")\n",
        "    summaries = []\n",
        "    successful_urls = []\n",
        "    for url in search_urls:\n",
        "        summary = scrape_and_summarize(url)\n",
        "        if summary:\n",
        "            summaries.append(summary)\n",
        "            successful_urls.append(url)\n",
        "\n",
        "    if not summaries:\n",
        "        return {\"verdict\": \"âŒ Likely Fake News (No usable evidence found online)\", \"proof_urls\": []}\n",
        "\n",
        "    evidence = \" \".join(summaries)\n",
        "    print(f'\\nðŸ’¡ Compiled Evidence: \"{evidence[:400]}...\"')\n",
        "\n",
        "    # 4. Verify the claim against the evidence using an NLI model\n",
        "    print(\"\\nðŸ§  Verifying claim against evidence...\")\n",
        "    # We can also force the NLI model to the CPU for maximum stability\n",
        "    nli_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=-1)\n",
        "\n",
        "    result = nli_classifier(evidence, candidate_labels=[main_claim])\n",
        "    top_score = result['scores'][0]\n",
        "    print(f\"NLI Result: The evidence score for the claim is {top_score:.2f}\")\n",
        "\n",
        "    # 5. Filter the successful URLs to find authentic ones\n",
        "    proof_urls = []\n",
        "    for url in successful_urls:\n",
        "        domain = urlparse(url).netloc\n",
        "        if domain.replace('www.', '') in AUTHENTIC_SOURCES:\n",
        "            proof_urls.append(url)\n",
        "\n",
        "    # 6. Make a final judgment and return it as a dictionary\n",
        "    if top_score > 0.70:\n",
        "        verdict = \"âœ… Likely Real News (Evidence supports the claim)\"\n",
        "    elif top_score < 0.30:\n",
        "        verdict = \"âŒ Likely Fake News (Evidence does not support the claim)\"\n",
        "    else:\n",
        "        verdict = \"ðŸ¤” Uncertain (Evidence is inconclusive)\"\n",
        "\n",
        "    return {\"verdict\": verdict, \"proof_urls\": proof_urls}"
      ],
      "metadata": {
        "id": "fPIFM_UY0Kcs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Test Case 1: A made-up, sensational claim ---\n",
        "fake_news_article = \"Body: Islamabad, Pakistan â€“ In a shocking move that has stunned global financial markets, the State Bank of Pakistan (SBP) has secretly finalized plans to phase out the Pakistani Rupee and introduce a new gold-backed digital currency, tentatively named the PakCoin. Sources from within the Ministry of Finance claim the transition, set to begin next year, will completely replace the Rupee by 2026. An unnamed official allegedly stated, This is the future. Every PakCoin will be digitally tied to a physical gram of gold held in our national reserves. This will eliminate inflation overnight and make Pakistan a global economic powerhouse. The report suggests a massive, undisclosed gold purchase from Russia and China is already underway to support the new currency. Financial experts are reportedly baffled, as no official announcement has been made through proper channels.\"\n",
        "\n",
        "print(\"ðŸ§ª Testing with a FAKE news article...\")\n",
        "result_data = detect_fake_news(fake_news_article)\n",
        "\n",
        "print(f\"\\\\n====================\\\\nFinal Verdict: The article is {result_data['verdict']}\\\\n====================\")\n",
        "if result_data['proof_urls']:\n",
        "    print(\"\\\\nSupporting evidence from authentic sources found:\")\n",
        "    for url in result_data['proof_urls']:\n",
        "        print(f\"- {url}\")\n",
        "else:\n",
        "    print(\"\\\\nNo supporting evidence from authentic sources was found.\")\n",
        "\n",
        "\n",
        "print(\"\\\\n\\\\n\" + \"=\"*50 + \"\\\\n\\\\n\")\n",
        "\n",
        "\n",
        "# --- Test Case 2: A plausible, real-world topic ---\n",
        "real_news_article = \"Headline: Pakistan Secures World Bank Financing for Karachi Water and Sewerage Improvement Project Body: Islamabad, Pakistan â€“ The Government of Pakistan has signed a financing agreement with the World Bank for the Karachi Water and Sewerage Services Improvement Project (KWSSIP) - Phase II. The project aims to improve access to safe water and sanitation services for residents of Karachi, Pakistan's largest city. The financing will support the Karachi Water and Sewerage Corporation (KWSC) in rehabilitating water distribution networks, upgrading pumping stations, and improving the city's wastewater treatment infrastructure. According to the Ministry of Economic Affairs, this initiative is crucial for enhancing climate resilience and public health in the sprawling metropolis. This project continues the efforts of Phase I, focusing on institutional reforms and infrastructure upgrades to create a more efficient and reliable water utility for the city's millions of residents.\"\n",
        "\n",
        "print(\"ðŸ§ª Testing with a REAL news article...\")\n",
        "result_data = detect_fake_news(real_news_article)\n",
        "\n",
        "print(f\"\\\\n====================\\\\nFinal Verdict: The article is {result_data['verdict']}\\\\n====================\")\n",
        "if result_data['proof_urls']:\n",
        "    print(\"\\\\nSupporting evidence from authentic sources found:\")\n",
        "    for url in result_data['proof_urls']:\n",
        "        print(f\"- {url}\")\n",
        "else:\n",
        "    print(\"\\\\nNo supporting evidence from authentic sources was found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7CQmFnP0MS7",
        "outputId": "b41752ee-0b83-4e9f-c967-0d8841d4a206"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Testing with a FAKE news article...\n",
            "--- Starting Intelligent Fake News Detection ---\n",
            "ðŸŽ¯ Extracting main claim from article...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Main Claim Identified: \" The State Bank of Pakistan (SBP) has secretly finalized plans to phase out the Pakistani Rupee and introduce a new gold-backed digital currency, tentatively named the PakCoin . Sources from within the Ministry of Finance claim the transition, set to begin next year, will completely replace\"\n",
            "\n",
            "ðŸŒ Searching the web for evidence...\n",
            "\n",
            "âœï¸ Summarizing evidence from top sources...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 100. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 70. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 71. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=35)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 56. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¡ Compiled Evidence: \" More than 20% of Pakistanâ€™s foreign direct investment in Pakistan;- 25% of the foreign capital in Pakistan . - Foreign direct investment by sector;- 50% of foreign loans to private sector business;- 70% of Pakistani Direct Direct Investment .  The State Bank of Pakistan (SBP) has officially announced that newly designed currency notes will begin circulating in Pakistan 2025 . The new currency not...\"\n",
            "\n",
            "ðŸ§  Verifying claim against evidence...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLI Result: The evidence score for the claim is 0.02\n",
            "\\n====================\\nFinal Verdict: The article is âŒ Likely Fake News (Evidence does not support the claim)\\n====================\n",
            "\\nSupporting evidence from authentic sources found:\n",
            "- https://www.sbp.org.pk/index.html\n",
            "- https://tribune.com.pk/story/2454809/sbp-all-set-to-replace-currency-notes-in-circulation-to-curb-corruption\n",
            "\\n\\n==================================================\\n\\n\n",
            "ðŸ§ª Testing with a REAL news article...\n",
            "--- Starting Intelligent Fake News Detection ---\n",
            "ðŸŽ¯ Extracting main claim from article...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸŽ¯ Main Claim Identified: \" Pakistan Secures World Bank Financing for Karachi Water and Sewerage Improvement Project . Project aims to improve access to safe water and sanitation services for residents of Karachi . Project continues efforts of Phase I, focusing on institutional reforms and infrastructure upgrades .\"\n",
            "\n",
            "ðŸŒ Searching the web for evidence...\n",
            "\n",
            "âœï¸ Summarizing evidence from top sources...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 78. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=39)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 57. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=28)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 64. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=32)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 61. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=30)\n",
            "Device set to use cpu\n",
            "Your max_length is set to 150, but your input_length is only 55. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=27)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ’¡ Compiled Evidence: \" World Bank has approved $240 million in financing for the Second Karachi Water and Sewerage Services Improvement Project (KWSSIP-2) The project will provide safely managed water supply to nearly 16 million people and sanitation services to nearly 7.5 million people in Karachi by 2030 .  World Bank has approved $240 million financing for the second Karachi Water and Sewerage Services Improvement P...\"\n",
            "\n",
            "ðŸ§  Verifying claim against evidence...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLI Result: The evidence score for the claim is 0.84\n",
            "\\n====================\\nFinal Verdict: The article is âœ… Likely Real News (Evidence supports the claim)\\n====================\n",
            "\\nSupporting evidence from authentic sources found:\n",
            "- https://www.worldbank.org/en/news/press-release/2024/12/11/pakistan-world-bank-approves-240-million-to-support-investments-in-water-sanitation-and-hygiene-services-in-karachi\n",
            "- https://www.dawn.com/news/1878611\n",
            "- https://tribune.com.pk/story/2515954/world-bank-approves-240m-to-improve-karachis-water\n"
          ]
        }
      ]
    }
  ]
}